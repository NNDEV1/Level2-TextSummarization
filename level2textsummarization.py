# -*- coding: utf-8 -*-
"""Level2TextSummarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qRg5Q-nZMJr3REYwe3hC9fslSxYb3Rx2
"""

#!pip install transformers

from urllib import request
from bs4 import BeautifulSoup as bs
import re
import nltk
import heapq
from transformers import pipeline

nltk.download('punkt')
nltk.download('stopwords')

#Website that data is scraped from can be changed
url = 'https://en.wikipedia.org/wiki/Toilet'
TotalContent = ""

htmlDoc = request.urlopen(url)

soupObj = bs(htmlDoc, 'html.parser')
paragraphContents = soupObj.findAll('p')

for content in paragraphContents:

  TotalContent += content.text



total_cleaned_content = re.sub(r'\[[0-9]*\]', ' ', TotalContent)
total_cleaned_content = re.sub(r'\s+', ' ', total_cleaned_content)

sentence_tokens = nltk.sent_tokenize(total_cleaned_content)

total_cleaned_content = re.sub(r'[^a-zA-Z]', ' ', total_cleaned_content)
total_cleaned_content = re.sub(r'\s+', ' ', total_cleaned_content)

print(sentence_tokens)

words_tokens = nltk.word_tokenize(total_cleaned_content)

stopwords = nltk.corpus.stopwords.words('english')

word_frequencies = {}

for word in words_tokens:

  if word not in stopwords:

    if word not in word_frequencies.keys():

      word_frequencies[word] = 1
    else:

      word_frequencies[word] += 1


print(words_tokens)

maximum_frequency = max(word_frequencies.values())

for word in word_frequencies.keys():
  word_frequencies[word] = (word_frequencies[word]/maximum_frequency)

print(word_frequencies)

sentence_scores = {}

for sentence in sentence_tokens:

  for word in nltk.word_tokenize(sentence.lower()):

    if word in word_frequencies.keys():

      if (len(sentence.split(' '))) < 30:

        if sentence not in sentence_scores.keys():
          sentence_scores[sentence] = word_frequencies[word]
        else:
          sentence_scores[sentence] += word_frequencies[word]


points = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)

total_text = ""

for i in points:

    total_text += i

summarizer = pipeline("summarization") # model="t5-small"

summary = summarizer(total_text, max_length=128, min_length=30, do_sample=False)

summary[0]["summary_text"]

print("Bullet Points: ")

for i in points:
    
   print("\n - " + i)

print("\n Summary: " + str(summary[0]["summary_text"]))

